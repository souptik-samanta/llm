from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load the model from Safetensors format
model = AutoModelForCausalLM.from_pretrained('./trained_model')

# Save model to a PyTorch file
torch.save(model.state_dict(), './trained_model_pytorch/pytorch_model.bin')

# Load tokenizer and save it
tokenizer = AutoTokenizer.from_pretrained('./trained_model')
tokenizer.save_pretrained('./trained_model_pytorch')





first the above part then below it 

see below















from transformers import AutoTokenizer, AutoModelForCausalLM

# Define paths
safetensors_model_path = './trained_model'
pytorch_model_path = './trained_model_pytorch'

# Load tokenizer and model from Safetensors directory
tokenizer = AutoTokenizer.from_pretrained(safetensors_model_path)
model = AutoModelForCausalLM.from_pretrained(safetensors_model_path)

# Ensure model is loaded properly
print("Model architecture:", model)

# Save model and tokenizer in PyTorch format
model.save_pretrained(pytorch_model_path)
tokenizer.save_pretrained(pytorch_model_path)

# Verify saved files
saved_files = os.listdir(pytorch_model_path)
print("Files in the PyTorch model directory:", saved_files)

# Check specifically for pytorch_model.bin
pytorch_model_bin_path = os.path.join(pytorch_model_path, 'pytorch_model.bin')
if os.path.exists(pytorch_model_bin_path):
    print(f"Found PyTorch model file at: {pytorch_model_bin_path}")
else:
    print("PyTorch model file not found.")
