from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments
from datasets import Dataset
from google.colab import files

# Upload the dataset file
uploaded = files.upload()

# Load the dataset (assuming the uploaded file is julius_caesar.txt)
def load_text_dataset(file_name):
    with open(file_name, "r", encoding="utf-8") as f:
        text = f.read().split("\n")
    # Create a Hugging Face Dataset object
    return Dataset.from_dict({"text": text})

# Load the tokenizer and model
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Add padding token if not present
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    model.resize_token_embeddings(len(tokenizer))

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

# Main function to load data, tokenize, and train the model
def main():
    # Load dataset from uploaded file
    dataset = load_text_dataset("julius_caesar.txt")

    # Tokenize the dataset using map for efficient processing
    tokenized_dataset = dataset.map(tokenize_function, batched=True)

    # Data collator for dynamic padding during training
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    # Define training arguments
    training_args = TrainingArguments(
        output_dir="./results",
        evaluation_strategy="no",
        learning_rate=2e-5,
        per_device_train_batch_size=4,
        num_train_epochs=3,  # Increase epochs for better fine-tuning
        weight_decay=0.01,
        save_total_limit=2,
        logging_dir='./logs',
        logging_steps=10,
    )

    # Initialize the trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
    )

    # Train the model
    trainer.train()

    # Save the model
    model.save_pretrained('./trained_model')
    tokenizer.save_pretrained('./trained_model')

if __name__ == "__main__":
    main()
